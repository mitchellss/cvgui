<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>cvgui.inputs.computer_vision.computer_vision API documentation</title>
<meta name="description" content="Generates poses based on a computer vision model and a frame input." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cvgui.inputs.computer_vision.computer_vision</code></h1>
</header>
<section id="section-intro">
<p>Generates poses based on a computer vision model and a frame input.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Generates poses based on a computer vision model and a frame input.&#34;&#34;&#34;
import cv2
import numpy as np
import multiprocessing as mp
from cvgui.core.recieving.service import CVModel, FrameInput


class ComputerVisionPose:
    &#34;&#34;&#34;Generates poses based on a computer vision model and a frame input.&#34;&#34;&#34;

    def __init__(self, frame_input: FrameInput, model: CVModel):
        &#34;&#34;&#34;Creates a new pose generator based on a computer vision
        model.

        Args:
            frame_input (FrameInput): The input of images to the computer
            vision model.
            model (CVModel): The model use to interpret the images from
            the frame input.
        &#34;&#34;&#34;
        self.frame_input: FrameInput = frame_input
        self.model: CVModel = model

    def start(self, skeleton_queue: mp.Queue):
        &#34;&#34;&#34;
        Starts two processes, one for capturing/displaying frame input data and
        one for processing that frame input data to get skeletons out of them.
        These are done as separate processes because otherwise the skeleton
        processing greatly slows down the speed at which frames are collected,
        resulting in video feedback that is &#34;laggy&#34;.

        Args:
            skeleton_queue (multiprocessing.Queue): The queue to put skeleton
                data into once it has been processed from frames.

        Returns:
            list[multiprocessing.Process]: All the processes started by this 
                method so they can be closed correctly later down the line.
        &#34;&#34;&#34;
        image_queue: mp.Queue = mp.Queue()
        print(&#34;Starting image processing pipeline &#34;
              &#34;(This might take a while on Windows)...&#34;)
        cap = mp.Process(target=self.capture_and_show, args=(image_queue,))
        proc = mp.Process(target=self.process_image,
                          args=(image_queue, skeleton_queue))
        cap.start()
        proc.start()
        return [cap, proc]

    def capture_and_show(self, image_queue):
        &#34;&#34;&#34;
        Infinitely retrieves new frames and places them in the image
        queue. Additionally, displays incoming frames to the user in
        real-time.
        &#34;&#34;&#34;
        while True:
            frame = self.frame_input.get_frame()
            image_queue.put(frame)
            cv2.imshow(&#34;Video Input&#34;, frame)
            wait_key = cv2.waitKey(1)
            if wait_key == 27:
                pass

    def process_image(self, image_queue, skeleton_queue):
        &#34;&#34;&#34;
        Infinitely takes images from the given queue and turns them into 
        skeleton data using a computer vision model.
        &#34;&#34;&#34;
        while True:
            if image_queue.empty():
                continue
            skeleton = self.model.get_pose(image_queue.get())
            skeleton_queue.put(skeleton)

    def get_pose(self) -&gt; np.ndarray:
        &#34;&#34;&#34;Uses the frame input and computer vision model in tandem
        to generate a single pose.&#34;&#34;&#34;
        frame: np.ndarray = self.frame_input.get_frame()
        # Shaves about 5ms off each frame by passing by reference, not value
        frame.flags.writeable = False
        pose: np.ndarray = self.model.get_pose(frame)
        return pose</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose"><code class="flex name class">
<span>class <span class="ident">ComputerVisionPose</span></span>
<span>(</span><span>frame_input: <a title="cvgui.core.recieving.service.FrameInput" href="../../core/recieving/service.html#cvgui.core.recieving.service.FrameInput">FrameInput</a>, model: <a title="cvgui.core.recieving.service.CVModel" href="../../core/recieving/service.html#cvgui.core.recieving.service.CVModel">CVModel</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates poses based on a computer vision model and a frame input.</p>
<p>Creates a new pose generator based on a computer vision
model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>frame_input</code></strong> :&ensp;<code>FrameInput</code></dt>
<dd>The input of images to the computer</dd>
<dt>vision model.</dt>
<dt><strong><code>model</code></strong> :&ensp;<code>CVModel</code></dt>
<dd>The model use to interpret the images from</dd>
</dl>
<p>the frame input.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ComputerVisionPose:
    &#34;&#34;&#34;Generates poses based on a computer vision model and a frame input.&#34;&#34;&#34;

    def __init__(self, frame_input: FrameInput, model: CVModel):
        &#34;&#34;&#34;Creates a new pose generator based on a computer vision
        model.

        Args:
            frame_input (FrameInput): The input of images to the computer
            vision model.
            model (CVModel): The model use to interpret the images from
            the frame input.
        &#34;&#34;&#34;
        self.frame_input: FrameInput = frame_input
        self.model: CVModel = model

    def start(self, skeleton_queue: mp.Queue):
        &#34;&#34;&#34;
        Starts two processes, one for capturing/displaying frame input data and
        one for processing that frame input data to get skeletons out of them.
        These are done as separate processes because otherwise the skeleton
        processing greatly slows down the speed at which frames are collected,
        resulting in video feedback that is &#34;laggy&#34;.

        Args:
            skeleton_queue (multiprocessing.Queue): The queue to put skeleton
                data into once it has been processed from frames.

        Returns:
            list[multiprocessing.Process]: All the processes started by this 
                method so they can be closed correctly later down the line.
        &#34;&#34;&#34;
        image_queue: mp.Queue = mp.Queue()
        print(&#34;Starting image processing pipeline &#34;
              &#34;(This might take a while on Windows)...&#34;)
        cap = mp.Process(target=self.capture_and_show, args=(image_queue,))
        proc = mp.Process(target=self.process_image,
                          args=(image_queue, skeleton_queue))
        cap.start()
        proc.start()
        return [cap, proc]

    def capture_and_show(self, image_queue):
        &#34;&#34;&#34;
        Infinitely retrieves new frames and places them in the image
        queue. Additionally, displays incoming frames to the user in
        real-time.
        &#34;&#34;&#34;
        while True:
            frame = self.frame_input.get_frame()
            image_queue.put(frame)
            cv2.imshow(&#34;Video Input&#34;, frame)
            wait_key = cv2.waitKey(1)
            if wait_key == 27:
                pass

    def process_image(self, image_queue, skeleton_queue):
        &#34;&#34;&#34;
        Infinitely takes images from the given queue and turns them into 
        skeleton data using a computer vision model.
        &#34;&#34;&#34;
        while True:
            if image_queue.empty():
                continue
            skeleton = self.model.get_pose(image_queue.get())
            skeleton_queue.put(skeleton)

    def get_pose(self) -&gt; np.ndarray:
        &#34;&#34;&#34;Uses the frame input and computer vision model in tandem
        to generate a single pose.&#34;&#34;&#34;
        frame: np.ndarray = self.frame_input.get_frame()
        # Shaves about 5ms off each frame by passing by reference, not value
        frame.flags.writeable = False
        pose: np.ndarray = self.model.get_pose(frame)
        return pose</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.capture_and_show"><code class="name flex">
<span>def <span class="ident">capture_and_show</span></span>(<span>self, image_queue)</span>
</code></dt>
<dd>
<div class="desc"><p>Infinitely retrieves new frames and places them in the image
queue. Additionally, displays incoming frames to the user in
real-time.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def capture_and_show(self, image_queue):
    &#34;&#34;&#34;
    Infinitely retrieves new frames and places them in the image
    queue. Additionally, displays incoming frames to the user in
    real-time.
    &#34;&#34;&#34;
    while True:
        frame = self.frame_input.get_frame()
        image_queue.put(frame)
        cv2.imshow(&#34;Video Input&#34;, frame)
        wait_key = cv2.waitKey(1)
        if wait_key == 27:
            pass</code></pre>
</details>
</dd>
<dt id="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.get_pose"><code class="name flex">
<span>def <span class="ident">get_pose</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Uses the frame input and computer vision model in tandem
to generate a single pose.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pose(self) -&gt; np.ndarray:
    &#34;&#34;&#34;Uses the frame input and computer vision model in tandem
    to generate a single pose.&#34;&#34;&#34;
    frame: np.ndarray = self.frame_input.get_frame()
    # Shaves about 5ms off each frame by passing by reference, not value
    frame.flags.writeable = False
    pose: np.ndarray = self.model.get_pose(frame)
    return pose</code></pre>
</details>
</dd>
<dt id="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.process_image"><code class="name flex">
<span>def <span class="ident">process_image</span></span>(<span>self, image_queue, skeleton_queue)</span>
</code></dt>
<dd>
<div class="desc"><p>Infinitely takes images from the given queue and turns them into
skeleton data using a computer vision model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_image(self, image_queue, skeleton_queue):
    &#34;&#34;&#34;
    Infinitely takes images from the given queue and turns them into 
    skeleton data using a computer vision model.
    &#34;&#34;&#34;
    while True:
        if image_queue.empty():
            continue
        skeleton = self.model.get_pose(image_queue.get())
        skeleton_queue.put(skeleton)</code></pre>
</details>
</dd>
<dt id="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.start"><code class="name flex">
<span>def <span class="ident">start</span></span>(<span>self, skeleton_queue: <bound method BaseContext.Queue of <multiprocessing.context.DefaultContext object at 0x7faad07135e0>>)</span>
</code></dt>
<dd>
<div class="desc"><p>Starts two processes, one for capturing/displaying frame input data and
one for processing that frame input data to get skeletons out of them.
These are done as separate processes because otherwise the skeleton
processing greatly slows down the speed at which frames are collected,
resulting in video feedback that is "laggy".</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>skeleton_queue</code></strong> :&ensp;<code>multiprocessing.Queue</code></dt>
<dd>The queue to put skeleton
data into once it has been processed from frames.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[multiprocessing.Process]</code></dt>
<dd>All the processes started by this
method so they can be closed correctly later down the line.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start(self, skeleton_queue: mp.Queue):
    &#34;&#34;&#34;
    Starts two processes, one for capturing/displaying frame input data and
    one for processing that frame input data to get skeletons out of them.
    These are done as separate processes because otherwise the skeleton
    processing greatly slows down the speed at which frames are collected,
    resulting in video feedback that is &#34;laggy&#34;.

    Args:
        skeleton_queue (multiprocessing.Queue): The queue to put skeleton
            data into once it has been processed from frames.

    Returns:
        list[multiprocessing.Process]: All the processes started by this 
            method so they can be closed correctly later down the line.
    &#34;&#34;&#34;
    image_queue: mp.Queue = mp.Queue()
    print(&#34;Starting image processing pipeline &#34;
          &#34;(This might take a while on Windows)...&#34;)
    cap = mp.Process(target=self.capture_and_show, args=(image_queue,))
    proc = mp.Process(target=self.process_image,
                      args=(image_queue, skeleton_queue))
    cap.start()
    proc.start()
    return [cap, proc]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cvgui.inputs.computer_vision" href="index.html">cvgui.inputs.computer_vision</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose" href="#cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose">ComputerVisionPose</a></code></h4>
<ul class="">
<li><code><a title="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.capture_and_show" href="#cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.capture_and_show">capture_and_show</a></code></li>
<li><code><a title="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.get_pose" href="#cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.get_pose">get_pose</a></code></li>
<li><code><a title="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.process_image" href="#cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.process_image">process_image</a></code></li>
<li><code><a title="cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.start" href="#cvgui.inputs.computer_vision.computer_vision.ComputerVisionPose.start">start</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>